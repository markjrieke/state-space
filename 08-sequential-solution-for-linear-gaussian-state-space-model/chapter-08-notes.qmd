---
title: "Chapter 8: Sequential Solution for Linear Gaussian State-Space Model"
format: gfm
---

```{r}
library(tidyverse)
library(riekelib)
```


## 8.1 Kalman Filter

* The optimal sequential estimation method for the linear Gaussian state-space model is the *Kalman filter*. 
* Unlike the Wiener filter, the Kalman filter can handle non-stationary processes.

$$
\begin{align*}
x_t &= G_t x_{t-1} + w_t \\
y_t &= F_t x_t + v_t \\
w_t &\sim \text{Normal}(0, W_t) \\
v_t &\sim \text{Normal}(0, V_t) \\
x_0 &\sim \text{Normal}(m_0, C_0)
\end{align*}
$$

* $G_t$ is the $p \times p$ state transition matrix; $F_t$ is the $1 \times p$ observation matrix, $W_t$ is the $p \times p$ covariance matrix of the state noise, and $V_t$ is the variance of the observation noise.
* For time $t=0$, $m_0$ is the $p$-dimensional mean vector and $C_0$ is the $p \times p$ covariance matrix. 

### 8.1.1 Kalman Filtering

* Given distribution at time $t-1$: $m_{t-1}$, $C_{t-1}$:
* Update the *one step ahead predictive distribution*
  * (Mean): $a_t = G_t m_{t-1}$
  * (Covariance): $R_t = G_t C_{t-1} G_t^\top + W_t$
* Update the *one step ahead predictive likelihood*
  * (Mean): $f_t = F_t a_t$
  * (Covariance): $Q_t = F_t R_t F_t^\top + V_t$
* Update the *Kalman gain*
  * $K_t = R_t F_t^\top Q_t^{-1}$
* Update the *filtering distribution*
  * (Mean): $m_t = a_t + K_t [y_t - f_t]$
  * (Covariance): $C_t = [I - K_t F_t] R_t$

* In plain english:
  * *One step ahead predictive distribution*: the filtering distribution one time before is transitioned forward based on the state equation.
  * *One step ahead predictive likelihood*: the one step ahead predictive distribution is converted to the domain of observations.
  * *Filtering distribution*: the one step ahead predictive distribution is corrected based on the likelihood
  
* In code:

```{r}
# kalman filtering from scratch!

# flow data in the nile as observations, y
y <- Nile
t_max <- length(y)

# perform kalman filtering for a given time point t
kalman_filtering <- function(m_t_minus_1,
                             C_t_minus_1,
                             t) {
  
  # one step ahead predictive distribution
  a_t <- G_t %*% m_t_minus_1
  R_t <- G_t %*% C_t_minus_1 %*% t(G_t) + W_t
  
  # one step ahead predictive likelihood
  f_t <- F_t %*% a_t
  Q_t <- F_t %*% R_t %*% t(F_t) + V_t
  
  # kalman gain
  K_t <- R_t %*% t(F_t) %*% MASS::ginv(Q_t)
  
  # state update
  m_t <- a_t + K_t %*% (y[t] - f_t)
  C_t <- (diag(nrow(R_t)) - K_t %*% F_t) %*% R_t
  
  # return the mean/variance of the filtering & one-step-ahead predictive distributions
  out <- 
    list(
      m = m_t,
      C = C_t,
      a = a_t,
      R = R_t
    )
  
  return(out)
  
}

# set parameters for the the linear gaussian state space
# (all 1x1 matrices here)
G_t <- matrix(1)
W_t <- matrix(exp(7.29))
F_t <- matrix(1)
V_t <- matrix(exp(9.62))
m0 <- matrix(0)
C0 <- matrix(1e7)

# empty vectors for results
m <- rep(NA_real_, t_max)
C <- rep(NA_real_, t_max)
a <- rep(NA_real_, t_max)
R <- rep(NA_real_, t_max)

# time t=1
KF <- kalman_filtering(m0, C0, t = 1)
m[1] <- KF$m
C[1] <- KF$C # the colonel >:)
a[1] <- KF$a
R[1] <- KF$R

# remaining time points
for (t in 2:t_max) {
  KF <- kalman_filtering(m[t - 1], C[t - 1], t = t)
  m[t] <- KF$m
  C[t] <- KF$C
  a[t] <- KF$a
  R[t] <- KF$R
}

tibble(m, C, y) %>%
  rowid_to_column("idx") %>%
  mutate(sd = sqrt(C)) %>%
  normal_interval(m, sd) %>%
  select(-c(C, sd)) %>%
  ggplot(aes(x = idx)) + 
  geom_ribbon(aes(ymin = ci_lower,
                  ymax = ci_upper),
              alpha = 0.25,
              fill = "royalblue") +
  geom_line(aes(y = m),
            color = "royalblue") +
  geom_point(aes(y = y)) +
  theme_rieke(base_family = "sans")
```

* The log-likelihood for the entire series can be obtained from the one-step-ahead predictive likelihood:

$$
\begin{align*}
l(\theta) &= \sum_{t=1}^T \log p(y_t \ | \ y_{1:t-1}; \theta) \\
&= -\frac{1}{2} \sum_{t=1}^T \log | Q_t |- \frac{1}{2} \sum_{t=1}^T \frac{(y_t - f_t)^2}{Q_t}
\end{align*}
$$

### 8.1.2 Kalman Prediction

* Kalman prediction for $k$ step ahead predictive distribution: $\text{Normal}(a_t(k), R_t(k))$

```{r}
# kalman prediction from scratch! (assuming filtering already completed)

# prediction period
t <- t_max
n_ahead <- 10

kalman_prediction <- function(a_t0, R_t0) {
  
  # one step ahead predictive distribution
  a_t1 <- G_t_plus_1 %*% a_t0
  R_t1 <- G_t_plus_1 %*% R_t0 %*% t(G_t_plus_1) + W_t_plus_1
  
  return(list(a = a_t1, R = R_t1))
  
}

# set time invariant parameters
G_t_plus_1 <- G_t
W_t_plus_1 <- W_t 

# initialize empty vectors
a_ <- rep(NA_real_, t_max + n_ahead)
R_ <- rep(NA_real_, t_max + n_ahead)

# k = 0 (zero-step ahead prediction = filtering distribution)
a_[t + 0] <- m[t]
R_[t + 0] <- C[t]

# k = 1 to n_ahead
for (k in 1:n_ahead) {
  KP <- kalman_prediction(a_[t + k-1], R_[t + k-1])
  a_[t+k] <- KP$a
  R_[t+k] <- KP$R
}

tibble(m, C, y) %>%
  mutate(sd = sqrt(C)) %>%
  normal_interval(m, sd) %>%
  select(-c(C, sd)) %>% 
  bind_rows(tibble(a = a_, 
                   R = R_)) %>% 
  filter(!is.na(m) | !is.na(a)) %>%
  rename_with(~paste0("filter_", .x),
              .cols = starts_with("ci")) %>%
  mutate(sd = sqrt(R)) %>%
  normal_interval(a, sd) %>%
  rowid_to_column("idx") %>% 
  ggplot(aes(x = idx)) + 
  geom_ribbon(aes(ymin = filter_ci_lower,
                  ymax = filter_ci_upper),
              fill = "royalblue",
              alpha = 0.5) +
  geom_line(aes(y = m),
            color = "royalblue") +
  geom_point(aes(y = y)) +
  geom_line(aes(y = ci_lower),
            color = "royalblue",
            linetype = "dashed") +
  geom_line(aes(y = ci_upper),
            color = "royalblue",
            linetype = "dashed") +
  geom_line(aes(y = a),
            color = "royalblue",
            linetype = "dashed") +
  theme_rieke(base_family = "sans")
  
```

### 8.1.3 Kalman Smoothing

* This book focuses on fixed-interval smoothing & assumes that Kalman filtering through $T$ has been completed.
* The smoothing distribution for the linear Gaussian state-space model is also the normal distribution, and as such is $\text{Normal}(s_t, S_t)$.

```{r}
# kalman smoothing from scratch! (assuming filtering has been completed)

kalman_smoothing <- function(s_t_plus_1, S_t_plus_1, t) {
  
  # smoothing gain
  A_t <- C[t] %*% t(G_t_plus_1) %*% MASS::ginv(R[t+1])
  
  # state update
  s_t <- m[t] + A_t %*% (s_t_plus_1 - a[t+1])
  S_t <- C[t] + A_t %*% (S_t_plus_1 - R[t+1]) %*% t(A_t)
  
  # return the mean/variance of the smoothing distribution
  return(list(s = s_t, S = S_t))
  
}

# initialize empty vectors
s <- rep(NA_real_, t_max)
S <- rep(NA_real_, t_max)

# time point t = t_max
s[t_max] <- m[t_max]
S[t_max] <- C[t_max]

for (t in (t_max - 1):1) {
  KS <- kalman_smoothing(s[t+1], S[t+1], t)
  s[t] <- KS$s
  S[t] <- KS$S
}

tibble(y = y,
       s = s,
       S = S) %>%
  rowid_to_column("idx") %>%
  mutate(sd = sqrt(S)) %>%
  normal_interval(s, sd) %>%
  ggplot(aes(x = idx)) + 
  geom_ribbon(aes(ymin = ci_lower,
                  ymax = ci_upper),
              fill = "royalblue",
              alpha = 0.5) +
  geom_line(aes(y = s),
            color = "royalblue") + 
  geom_point(aes(y = y)) + 
  theme_rieke(base_family = "sans")
```

